# Ads
## [TikTok Creative Center](https://ads.tiktok.com/business/creativecenter/pc/en?rid=vcxdlcdh10o)
- [About TikTok Ads Manager](https://ads.tiktok.com/help/article/tiktok-ads-manager-intro?lang=en)
  * **Dashboard**: Allows you to track changes in performance and review ad account data such as active campaigns, budget spending, and charts for tracking ad performance over time.
  * **Campaign**: Review all campaigns, ad groups, and ads you have created.
  * **Tools**: You can choose from 6 options in the drop-down list to manage what goes into your ads and who you want to reach: Events, Creatives, Audiences, Catalogs, Comments, and Instant Pages.
  * **Analytics**: You can create custom reports, or use one of our reporting templates to go deeper into your ad insights. You can also schedule reports to generate at a specific time.

 - [Ads Manager Walk Through](https://ads.tiktok.com/i18n/dashboard/?aadvid=7393058749373546497)
   * Create a campaign by objectives 
    <img width="929" alt="Screenshot 2024-07-18 at 3 25 39 PM" src="https://github.com/user-attachments/assets/579dc9d2-6238-4df9-aa6f-039f9aa6e7a2">

   * Automated rules in Tools
    <img width="946" alt="Screenshot 2024-07-18 at 3 29 19 PM" src="https://github.com/user-attachments/assets/dcebcb93-c586-445b-b28e-94aa55a532aa">

   * Under [Analytics](https://ads.tiktok.com/i18n/audience-insight?aadvid=7393058749373546497)
    <img width="959" alt="Screenshot 2024-07-18 at 3 31 04 PM" src="https://github.com/user-attachments/assets/84ed57c3-5137-49bc-b2b0-f56b3b39f348">
    
- [Trending](https://ads.tiktok.com/business/creativecenter/pc/en?rid=vcxdlcdh10o)
  * Learn from the Top-Performance Ads
  * [Explore Top Ads on TikTok](https://ads.tiktok.com/business/creativecenter/inspiration/topads/pc/en?period=30&region=US)

- [About This Ad](https://ads.tiktok.com/business/creativecenter/topads/7340427969016102913/pc/en?countryCode=US&from=001110&period=30)
  * Ad performance Merics
     * **CTR** is the number of clicks that your ad receives divided by the number of times your ad is shown: clicks ÷ impressions = CTR. For example, if you had 5 clicks and 100 impressions, then your CTR would be 5%.
     * **CVR** is calculated by dividing the number of users who converted by the number of users who clicked on the ad, and then multiplying by 100. For example, if 1,000 users saw an ad and 15 users installed the advertised app, the CVR would be 1.5%.
     * **Click** 
     * **Conversion**
     * **Remain** 
    <img width="624" alt="Screenshot 2024-07-18 at 3 55 56 PM" src="https://github.com/user-attachments/assets/55942556-dd44-4eb2-a703-33424aea0049">

## [16 of the Best TikTok Tools to Improve Your Marketing](https://blog.hootsuite.com/tiktok-tools/)
## [广告创建与投放](https://school.oceanengine.com/product_help/content/668400000006/121544)
## [抖音广告设计与推送算法](https://www.admin5.com/article/20201110/974851.shtml)
### 1. Type of Ads & Billings
- 开屏广告
  * 打开APP时第一时间出现的全屏广告
  * 有更强的视觉冲击效果
  * 常见于流量和日活高的应用
    
  展现规则
  * 静态、动态和视频三类广告时间分别为3、4、5秒
  * 支持展示和落地页跳转
    
  计价方式
  * CPM (Cost per thousand / 千人成本): used to identify the cost of every 1,000 impressions on a particular ad
  * 按时间收费, 广告主根据投放时间出价，不受曝光次数影响
    
 
- 信息流广告
  * 信息流广告根据用户兴趣和上下文浏览来推送
  * 相比强曝光的广告形式更原生，定向更精准，
  * 可互动的特性也有利于用户加深品牌印象
    
  展现规则
  (一般是在第3-4位出现，按文字链跳转又分为以下三类。)
  * 落地页广告：跳转H5，常见于培训课程，广告主一般会利用表单来获取用户信息提高转化率。
  * 下载页广告：跳转应用下载页，常见于游戏、教育类APP。
  * 购物页广告：跳转电商平台店铺，常见于服饰、美妆、母婴等品牌
  
  计价方式
  * CPC（点击收费），竞价起步单价0.2元；
  * CPA（实际投放效果计价），按照电话咨询量、APP下载量、表单提交量进行计费。
  
- 挑战赛广告
  * 抖音挑战赛根据广告主需求进行定制推广，类似于话题，但设有专门的话题页面。
  * 常见的挑战赛常吸引用户使用指定贴纸拍摄上传视频，利用创意和互动来进行排名送出奖品，常见于美食类产品。

- 达人带货广告
  * 最初的达人带货限于视频植入或纯软广，抖音小店开通后，达人带货变得更加方便，在主屏上就可以进入小店完成下单转化。
  * 相比前几种广告，带货类广告的转化更加直接。

  展现规则
  * 视频植入或跳转抖音小店
 
  计价方式
   
   
----------------------------------------------------------------

[抖音和小红书都在引领流行趋势，二者有何不同？](https://www.niaogebiji.com/article-672863-1.html)
1. 内容形式: 视频VS笔记
2. 内容分发逻辑: 算法VS互动
3. 引领趋势源头：个人VS话题
4. 持续时间：长VS短
5. 覆盖人群：广泛VS垂直
6. 商业模式：电商VS广告
   
* 抖音的电商模式使其在流行趋势的快速变现上具有优势，而小红书的广告模式则更注重长期品牌价值的培养和用户的深度参与。
* 对于追求快速曝光和短期内实现销售转化的品牌，抖音是一个更为合适的选择；而对于那些注重品牌形象长期建设和深度用户关系维护的品牌，则可以考虑在小红书上进行更为深入的内容营销。

----------------------------------------------------------------

## System Design Interview: Design an Ad Click Aggregator 
https://www.hellointerview.com/learn/system-design/answer-keys/ad-click-aggregator

**Ad Click Aggregator** is a system that collects and aggregates data on ad clicks. It is used by advertisers to track the performance of their ads and optimize their campaigns. 

<img width="939" alt="Screenshot 2024-07-18 at 4 41 29 PM" src="https://github.com/user-attachments/assets/05aab247-9be3-4963-868f-ca76c24bf95a">



#### Functional Requirements 
1. Users can click on an ad and be redirected to the advertiser's website
2. Advertisers can query ad click metrics over time with a minimum granularity of 1 minute
3. Ad targeting
4. Ad serving
5. Cross device tracking
6. Integration with offline marketing channels

#### Non-Functional Requirements
1. Scalable to support a peak of 10k clicks per second
2. Low latency analytics queries for advertisers (sub-second response time)
3. Fault tolerant and accurate data collection. We should not lose any click data.
4. As realtime as possible. Advertisers should be able to query data as soon as possible after the click.
5. Idempotent click tracking. We should not count the same click multiple times.
(Optional)
6. Fraud or spam detection
7. Demographic and geo profiling of users
8. Conversion tracking

<img width="937" alt="Screenshot 2024-07-18 at 4 42 10 PM" src="https://github.com/user-attachments/assets/8549bb28-8d55-4a58-a204-f31c174e68ea">


#### API / System Interface
* Input: 
  1. Click Data
  2. Advertiser Query
   
* Output:    
  1. Redirect 
  2. Aggregrated Click Metrcis

### Data Flow 
1. User clicks on an ad on a website.
2. The click is tracked and stored in the system.
3. The user is redirected to the advertiser's website.
4. Advertisers query the system for aggregated click metrics.


### High-Level Design
#### 1) Users can click on ads and be redirected to the target
When a user clicks on an ad which was placed by the **Ad Placement Service**, we will send a request to our `/click` endpoint, which will track the click and then redirect the user to the advertiser's website.

 * Two ways we can handle this redirect
 1) **Client side redirect:** When a user clicks on the ad, the browser will automatically redirect them to the target URL. The downside with this approach is that users could go to an advertiser's website without us knowing about it.
 2) **Server side redirect:** A more robust solution is to have the user click on the ad, which will then send a request to our server. Our server can then track the click and respond with a redirect to the advertiser's website via a 302 (redirect) status code. This way, we can ensure that we track every click and provide a consistent experience for users and advertisers. This approach also allows us to append additional tracking parameters to the URL
    
<img width="939" alt="Screenshot 2024-07-18 at 5 52 25 PM" src="https://github.com/user-attachments/assets/0327e240-a51c-4bd3-b164-720005f171c3">


#### 2) Advertisers can query ad click metrics over time at 1 minute intervals
Our users were successfully redirected, now let's focus on the advertisers. They need to be able to quickly query metrics about their ads to see how they're performing.

### *A simple design* will be have a single Click DB send data to the query service; 
There are lots of DB can be choosed, one way is to use Cassandra/(Amazon Keyspaces), where it was `write` optimazied in which it can support fast insertion & table updates.

<img width="941" alt="Screenshot 2024-07-18 at 5 15 08 PM" src="https://github.com/user-attachments/assets/85ed329e-e212-4f5b-92b4-f419e84081f7">


### *A better design* is to separate Analytics Database with Batch Processing

When a click comes in, we will store the raw event in our event database. Then, in batches, we can process the raw events and aggregate them into a separate database that is special optimized for querying. When an advertiser wants to query metrics, we simply query this analytics database for the metrics that they need. This allows us to provide low latency queries since we did the expensive aggregation work in advance.

> How much data will we be processing exactly? If we have 10k clicks per second and we choose to run our batch processing every 5 minutes, we will be processing 3M events every minute. Each event will only be a hundred bytes at most, so we will be processing 300MB of data every minute. This is well within the capabilities of Spark.

To **reduce contention** (aka. resource competiton) for separating heavy database writes/reads, and to support **fault isolation** in which 1 database down will not affect the other database serving.

<img width="939" alt="Screenshot 2024-07-18 at 5 13 38 PM" src="https://github.com/user-attachments/assets/2bf2f656-4717-4208-8f8a-db688780aa82">

Using map-reduce, **Spark** will read the raw events in parallel chunks, aggregate them by ad ID and minute timestamp, and then write the aggregated data to our analytics database.

For an analytics database, we want a technology that is optimized for reads and aggregations. Online analytical processing **(OLAP) databases like Redshift, Snowflake, or BigQuery are all good choices here. They are optimized for these types of queries and can handle the large volume of data that we will be storing.

#### 2) How can we ensure that we don't lose any click data?

The first thing to note is that we are already using a stream like **Kafka** or Kinesis to store the click data. By default, these streams are distributed, fault-tolerant, and highly available. They replicate data across multiple nodes and data centers, so even if a node goes down, the data is not lost. Importantly for our system, they also allow us to enable persistent storage, so even if the data is consumed by the stream processor, it is still stored in the stream for a certain period of time.

![ddd](https://github.com/user-attachments/assets/91ddf63c-983d-4132-93f3-ead6b8759986)

We can configure a *retention period of 7 days*, for example, so that if, for some reason, our stream processor goes down, it will come back up and can read the data that it lost from the stream again.

Stream processors like Flink also have a feature called *checkpointing*. This is where the processor periodically writes its state to a persistent storage like **S3**. If it goes down, it can read the last checkpoint and resume processing from where it left off. 

<img width="920" alt="Screenshot 2024-07-18 at 6 04 16 PM" src="https://github.com/user-attachments/assets/3cf7ceda-127d-416c-afe4-0e0c8f24ee84">

### *Reconciliation* matters!
> Click data matters, a lot. If we lose click data, we lose money.

We need to make sure that our data is correct. This is a tough balance, because guaranteeing correctness and low latency are often at odds. We can balance the two by introducing *periodic reconciliation*.

Despite our best efforts with the above measures, things could still go wrong. Transient processing errors in Flink, bad code pushes, out-of-order events in the stream, etc., could all lead to slight inaccuracies in our data. To catch these, we can introduce a *periodic reconciliation job that runs every hour or day*.

![kk](https://github.com/user-attachments/assets/04a4c64d-8432-4543-9e4c-c26dcbf9963f)

At the end of the stream, alongside the stream processors, we can also dump the raw click events to a data lake like S3. **Flink** supports this through its FileSystem interface and various **connectors**, allowing for both batch and **real-time data processing output**s to be stored directly in **S3 buckets**. Then, as with the "good" answer in **"Advertisers can query ad click metrics over time at 1-minute intervals"** above, we can run a **batch job** that reads all the raw click events from the data lake and re-aggregates them. This way, we can **compare** the results of the batch job to the results of the stream processor and ensure that they match.

This essentially combines our two solutions, real-time stream processing and periodic batch processing, to ensure that our data is not only fast but also accurate.

### 3) How can we prevent abuse from users clicking on ads multiple times? 
> Generate a Unique impression ID

**Ad Placement Service** generate a *unique impression ID for each ad instance shown to the user*. This impression ID would be sent to the browser along with the ad and will serve as an idempotency key. When the user clicks on the ad, the browser sends the impression ID along with the click data. This way we can **dedup** clicks based on the impression ID.

> What is an Impression? An ad impression is a metric that represents the display of an ad on a web page. It is counted when the ad is fetched from its source and displayed on the user's screen. If we showed the same user the same ad multiple times in multiple places, then each of these placements is a new impression.

#### How do we dedup?
We should dedup *before* we put the click in the stream (Flink). When a click comes in, we check if the impression ID exists in a **cache**. If it does, then its a duplicate and we ignore it. If it doesn't, then we put the click in the stream and add the impression ID to the cache.

Morever, if a malicious user could send a bunch of fake clicks with falsified impression IDs, we can *sign the impression ID with a secret key* before sending it to the browser. When the click comes in, we can verify the signature to ensure the impression ID is valid before checking the cache.

![mmm](https://github.com/user-attachments/assets/12c5d36e-6557-4135-b2ec-dabcf55c9f13)

Let's recap:
* **Ad Placement Service** generates a **unique impression ID** for each ad instance shown to the user.
* The impression ID is signed with **a secret key** and sent to the browser along with the ad.
* When the user clicks on the ad, the browser sends the impression ID along with the click data.
* **The Click Processor** verifies the **signature** of the impression ID.
* **The Click Processor** checks if the impression ID **exists in a cache**. If it does, then it's a duplicate, and we ignore it. If it doesn't, then we put the click in the stream and add the impression ID to the cache.

### 4) How can we ensure that advertisers can query metrics at low latency?

Query can still be slow is when we are aggregating over **larger time windows**, like a days, weeks, or even years. In this case, we can **pre-aggregate** the data in the **OLAP database**. This can be done by creating a new table that stores the aggregated data at a higher level of granularity, like daily or weekly.

> Pre-aggregating the data in the OLAP database is a common technique to improve query performance. It can be thought of similar to a form of caching. We are trading off storage space for query performance for the most common queries.




